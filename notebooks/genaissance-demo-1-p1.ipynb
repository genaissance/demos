{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07173af7-4b17-472a-8cd9-5532f6237a4c",
   "metadata": {},
   "source": [
    "# Project GenAIssance Demo\n",
    "\n",
    "This demonstration allows you to scrape data from a webiste, or a subsection of a website, ingest it into a in-memory vector database, and then provide a natural language query that is sent to the chatgpt API along with context from your data. This allows you to access the power of chatgpt for use with your data.\n",
    "\n",
    "**Important:** This demo is running in a limited environment and should not be used for very large websites. If you have a larger website, you can use the filter options described below to limit the scope of your job. You can use this demo effectively for up to a few hundred pages. If you attempt to scrape a very large amount of content, it may run slowly or fail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887c255-1ed3-467e-8a80-43ace1428d5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Scrape the web content to gather your data\n",
    "\n",
    "In the first step, we will use the scrapy python library to capture data from your website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b29a1-ebce-49cf-8e8f-9146ddb6cf10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### About input variables\n",
    "\n",
    "In ALL cases, this demo requires that the user inputs their OPENAI_API_KEY when prompted in the exercises below.\n",
    "\n",
    "In the sections below, you can execute the code with the example variables provided, or you can enter your own desired values to customize the demo with your desired data. If you would like to customize the demo with your data, please read this explanation to understand the values you will need to provide. \n",
    "\n",
    "#### The ALLOWED_DOMAINS variable\n",
    "\n",
    "This should be the url for the website or subsite you want to gather data from. If you have a more specific url that is better, for example, if you want to access content only from the vmware documentation, you would set your ALLOWED_DOMAIN to docs.vmware.com.\n",
    "\n",
    "#### The START_URL variable\n",
    "\n",
    "This is the seed URL that the web scraper will use as its starting point. If you are targeting an entire website, you could just enter the homepage URL for the website. But if you are targeting a subsection within a website, such as the documentation for a specific product, you should use the main page for the subsection you want to target. For example if you wanted to collect the TAP 1.4 documentation, a good URL to use as the START_URL value would be the main page for the TAP 1.4 documentation: `https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.4/tap/overview.html`.\n",
    "\n",
    "The scraper will start with this url, and then recursively scan all URLS on this page within the ALLOWED_DOMAIN, and recursively scan each discovered page until it has identified all page URLs in the ALLOWED_DOMAIN, subject to the ALLOW_RULE as described below\n",
    "\n",
    "#### The ALLOW_RULE Variable\n",
    "\n",
    "The ALLOW_RULE is a substring of a URL that allows you to limit the scope of the pages the scraper will index. For example, assume you want to index the Tanzu Application (TAP) 1.4 documentation, you will find that all the page URL's within the TAP 1.4 docs include a common pattern, for example:\n",
    "\n",
    "The TAP 1.4 Documentation main page URL is: `https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.4/tap/overview.html`. This URL includes the substring `en/VMware-Tanzu-Application-Platform/1.4`. If you look through the TAP documentation, you will find that all English language pages within the TAP 1.4 docs have this same URL pattern. Accordingly, an ALLOW_RULE value that would capture the TAP 1.4 documentation would be `en/VMware-Tanzu-Application-Platform/1.4/*`\n",
    "\n",
    "The ALLOW_RULE restricts the scraper from finding pages that do not include this pattern. The scraper starts crawling your site with the START_URL. Accordingly, if you were to use a START_URL of `docs.vmware.com` with the ALLOW_RULE `en/VMware-Tanzu-Application-Platform/1.4/*`, it would not work because there are no direct links to the TAP 1.4 Docs from the homepage, and the ALLOW_RULE prevents the scraper from crawling any links that do not match the ALLOW_RULE pattern.\n",
    "\n",
    "#### The PARSING_RULE variable (Optional)\n",
    "\n",
    "If desired, the parsing rule will allow you to filter the data you gather based on an additional substring in the URL. For example, assume you wanted to gather all the pages in the TAP 1.4 documentation specific to installation. There are many different ways to install TAP, and there are several different pages and subsections related to installation. You may find that the string \"install\" is present in all of the URL's you want. If you set the PARSING_RULE to `*install*`, it would only return URLs that included the substring `install` within the URL\n",
    "\n",
    "#### The BODY_TAG variable (Optional)\n",
    "\n",
    "Your results will be most optimal if you can focus in on the main sections within the webpages that you want to gather. Web pages have tons of metadata, and while this model will work well even with less-clean data, but, the cleaner the data the better. In most cases, you probably want the main body of visible text from your web page data. Most organized websites will use an html or css tag to identify the main body of text on every page. For example on docs.vmware.com, the main body of text on every page is wrapped inside a div with the tag \"div.rhs-center.article-wrapper\". This same tag will not work on other websites, as each website will have their own tagging schemes. If you do not know of a tag like this that will work for your desired site, you can just leave this blank, the demo code still does a very good job of identifying the relevant content from your input data for queries. \n",
    "\n",
    "The BODY_TAG variable is used after your site is crawled and the urls you want have been downloaded. There is a parse function that cleans the gathered data, and as part of that process, it can extract content that has the tag you specify with the BODY_TAG variable. This allows the cleaning job to easily discard all of the other irrelevant data like side nav bars and twitter links, headers/footers etc so the result only includes the main body of visible text on each page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab799f-4ac9-4e6f-a34a-252d3f883467",
   "metadata": {},
   "source": [
    "### How the scraper works in this demo\n",
    "\n",
    "Assuming you use the default input variables:\n",
    "```\n",
    "os.environ['ALLOWED_DOMAINS'] = 'docs.vmware.com'\n",
    "os.environ['START_URL'] = 'https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.4/tap/cli-plugins-tanzu-cli.html'\n",
    "os.environ['ALLOW_RULE'] = 'en/VMware-Tanzu-Application-Platform/1.4/*'\n",
    "os.environ['PARSING_RULE'] = ''\n",
    "os.environ['BODY_TAG'] = 'div.rhs-center.article-wrapper'\n",
    "```\n",
    "\n",
    "The scraper will start crawling the start URL and gather all URLs it can find subject to the ALLOWED_DOMAINS and ALLOW_RULE parameters, and will compile a list of all URL's that need to be collected. The UrlScraperSpider class then calls the HtmlScraperSpider class which downloads and cleans the content from each of the URLs identified, and can optionally enhance cleaning of the data with a BODY_TAG if provided. The example input variables will scrape the tanzu application platform 1.4 documentation from the vmware documentation website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a98375-d885-4fa4-80dc-f265bc798687",
   "metadata": {},
   "source": [
    "### Execute the scraping logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711a3b8-be2f-4feb-a603-343be422125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following installs are required. Please execute this code block without changing anything.\n",
    "!pip install scrapy\n",
    "!pip install crochet\n",
    "message = \"The required libraries have been installed\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314d5c6-e7c3-4e80-8f6b-4ac8bd6007cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following imports are required. Please execute this code block without changing anything.\n",
    "import os\n",
    "import scrapy\n",
    "import logging\n",
    "from crochet import setup, wait_for\n",
    "from scrapy.utils.log import configure_logging\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy import signals\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Initialize crochet\n",
    "setup()\n",
    "\n",
    "# Configure Scrapy logging\n",
    "configure_logging()\n",
    "logging.getLogger(\"scrapy\").setLevel(logging.INFO)\n",
    "\n",
    "message = \"The dependencies have been imported\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8d385-e593-46d6-bafa-7c78847fc698",
   "metadata": {},
   "source": [
    "_The following code block contains pre-populated input values that you may want to change. If you want to use your own data source, please modify the values below before executing._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ba6c9-3490-4238-960d-f3440092a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a container we would have typically exported these envars in the shell directly, but since this notebook is just running in python, we add this extra step to define the vars, while doing so as envars so as to preserve the rest of the code to run the same as it would server-side\n",
    "os.environ['ALLOWED_DOMAINS'] = 'docs.vmware.com'\n",
    "os.environ['START_URL'] = 'https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/overview.html'\n",
    "os.environ['ALLOW_RULE'] = 'en/VMware-Tanzu-Application-Platform/1.5/*'\n",
    "os.environ['PARSING_RULE'] = ''\n",
    "os.environ['BODY_TAG'] = 'div.rhs-center.article-wrapper'\n",
    "message = \"The Envars have been loaded\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc35554-fc62-45c6-bd36-48b6a62fba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code block without changing anything. \n",
    "allowed_domains = os.environ.get('ALLOWED_DOMAINS')\n",
    "start_url = os.environ.get('START_URL')\n",
    "allow_rule = os.environ.get('ALLOW_RULE')\n",
    "body_tag = os.environ.get('BODY_TAG')\n",
    "parsing_rule = os.environ.get('PARSING_RULE')\n",
    "message = \"The Envars have been imported\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf03fc-3c9a-43d0-981d-494836a5b46c",
   "metadata": {},
   "source": [
    "_The following code block is the UrlScraperSpider class. You do not need to change anything, just execute it to load the function and we will call it in a subsequent step._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50caf585-b911-47e5-82a2-2060752c5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrlScraperSpider(CrawlSpider):\n",
    "    name = 'url_scraper'\n",
    "\n",
    "    def __init__(self, allowed_domains, start_url, allow_rule, parsing_rule=None, *args, **kwargs):\n",
    "        self.allowed_domains = [allowed_domains]\n",
    "        self.start_urls = [start_url]\n",
    "        self.allow_rule = [allow_rule]\n",
    "        self.rules = (\n",
    "            Rule(LinkExtractor(allow=allow_rule), callback='parse_item', follow=True),\n",
    "        )\n",
    "        self.parsing_rule = parsing_rule\n",
    "        self.urls = set()  \n",
    "\n",
    "        super(UrlScraperSpider, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self._compile_rules()\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = super(UrlScraperSpider, cls).from_crawler(crawler, *args, **kwargs)\n",
    "        crawler.signals.connect(spider.spider_opened, signal=signals.spider_opened)\n",
    "        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n",
    "        return spider\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        logging.info(\"UrlScraperSpider started...\")\n",
    "\n",
    "    def spider_closed(self, spider):\n",
    "        logging.info(\"UrlScraperSpider finished.\")\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        links = LinkExtractor(allow=self.allow_rule).extract_links(response)\n",
    "        if self.parsing_rule:\n",
    "            links = LinkExtractor(allow=self.parsing_rule).extract_links(response)\n",
    "            \n",
    "        for link in links:\n",
    "            url = urlparse(link.url)._replace(fragment='').geturl()\n",
    "            self.urls.add(url)\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def closed(self, reason):\n",
    "        with open('urls.txt', 'w') as f:\n",
    "            f.write('\\n'.join(sorted(self.urls)))\n",
    "message = \"The UrlScraperSpider class has been loaded\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee4166-c3d1-4d1a-8fd0-7e2f570a8f13",
   "metadata": {},
   "source": [
    "_Now load the HtmlScraperSpider Class, which when called will downloads and clean the urls_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae2860-5567-40b2-a20a-e50b2ae838fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code block, do not change anything\n",
    "class HtmlScraperSpider(scrapy.Spider):\n",
    "    name = 'html_scraper'\n",
    "\n",
    "    def __init__(self, allowed_domains, body_tag=None, *args, **kwargs):\n",
    "        super(HtmlScraperSpider, self).__init__(*args, **kwargs)\n",
    "        self.allowed_domains = [allowed_domains]\n",
    "        os.makedirs('html_downloads', exist_ok=True)\n",
    "        self.start_urls = self.get_start_urls()\n",
    "        self.body_tag = body_tag\n",
    "\n",
    "    def get_start_urls(self):\n",
    "        with open('urls.txt', 'r') as f:\n",
    "            urls = f.read().split('\\n')\n",
    "        return urls\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = super(HtmlScraperSpider, cls).from_crawler(crawler, *args, **kwargs)\n",
    "        crawler.signals.connect(spider.spider_opened, signal=signals.spider_opened)\n",
    "        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n",
    "        return spider\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        logging.info(\"HtmlScraperSpider started...\")\n",
    "\n",
    "    def spider_closed(self, spider):\n",
    "        logging.info(\"HtmlScraperSpider finished.\")\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = os.path.join('html_downloads', response.url.split('/')[-1])\n",
    "\n",
    "        if self.body_tag:\n",
    "            content = response.css(self.body_tag).get()\n",
    "        else:\n",
    "            content = response.body\n",
    "\n",
    "        if content:\n",
    "            content = Selector(text=content).xpath('//text()').getall()\n",
    "            content = ''.join(content).strip()\n",
    "\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(content)\n",
    "\n",
    "        return {}\n",
    "message = \"The HtmlScraperSpider class has been loaded\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11668d3-b878-45ee-a02a-8890279e03d9",
   "metadata": {},
   "source": [
    "_Now that the classes have been loaded, call them by executing the code block below:_\n",
    "\n",
    "__Note:__ The command block below could take a minute or two to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca0e17-c80d-4da4-9ec9-aba9e702bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = CrawlerRunner(get_project_settings())\n",
    "\n",
    "@wait_for(timeout=None)\n",
    "def run_spider(spider_class, **kwargs):\n",
    "    return runner.crawl(spider_class, **kwargs)\n",
    "\n",
    "# Run the spiders\n",
    "run_spider(UrlScraperSpider, allowed_domains=allowed_domains, start_url=start_url, allow_rule=allow_rule, parsing_rule=parsing_rule)\n",
    "run_spider(HtmlScraperSpider, allowed_domains=allowed_domains, body_tag=body_tag)\n",
    "message = \"The spiders have completed running\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ca088-c6d0-4433-99b3-265e943122ab",
   "metadata": {},
   "source": [
    "### Review Step1 Outputs\n",
    "\n",
    "Congratulations, you made it through step 1! You have now downloaded and cleaned your input data!\n",
    "\n",
    "- In the left nav bar, you should see a file named \"urls.txt\". This file was created by the UrlSpiderScraper class and contains a list of all the URLS that were identified given your input criteria. Double click on the file to view it. \n",
    "- In the left nav bar, you should also see the html_downloads directory, double click on it to view the html files that were downloaded.\n",
    "  - Double click on some of the html files, you will see it is actually now just plaintext as it has beel cleaned for the next step, vectorizing the data\n",
    "  - Click on the folder icon in the nav bar, so you can see the root folder contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccfd034-b814-44d8-86b1-698704741071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
